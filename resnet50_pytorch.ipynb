{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-19T19:39:34.447145Z",
     "start_time": "2024-11-19T19:39:19.876593Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "from torch.utils.data import random_split\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0   x1   y1    x2    y2  class  \\\n",
      "0           0   39  116   569   375     14   \n",
      "1           1   36  116   868   587      3   \n",
      "2           2   85  109   601   381     91   \n",
      "3           3  621  393  1484  1096    134   \n",
      "4           4   14   36   133    99    106   \n",
      "\n",
      "                       ture_class_name      image  \n",
      "0                  Audi TTS Coupe 2012  00001.jpg  \n",
      "1                  Acura TL Sedan 2012  00002.jpg  \n",
      "2           Dodge Dakota Club Cab 2007  00003.jpg  \n",
      "3     Hyundai Sonata Hybrid Sedan 2012  00004.jpg  \n",
      "4  Ford F-450 Super Duty Crew Cab 2012  00005.jpg  \n",
      "    x1   y1   x2   y2  class                      ture_class_name        image\n",
      "0   30   52  246  147    181              Suzuki Aerio Sedan 2007  '00001.jpg'\n",
      "1  100   19  576  203    103  Ferrari 458 Italia Convertible 2012  '00002.jpg'\n",
      "2   51  105  968  659    145                Jeep Patriot SUV 2012  '00003.jpg'\n",
      "3   67   84  581  407    187              Toyota Camry Sedan 2012  '00004.jpg'\n",
      "4  140  151  593  339    185             Tesla Model S Sedan 2012  '00005.jpg'\n",
      "   class                      ture_class_name      image\n",
      "0     14                  Audi TTS Coupe 2012  00001.jpg\n",
      "1      3                  Acura TL Sedan 2012  00002.jpg\n",
      "2     91           Dodge Dakota Club Cab 2007  00003.jpg\n",
      "3    134     Hyundai Sonata Hybrid Sedan 2012  00004.jpg\n",
      "4    106  Ford F-450 Super Duty Crew Cab 2012  00005.jpg\n",
      "   class                      ture_class_name      image\n",
      "0    181              Suzuki Aerio Sedan 2007  00001.jpg\n",
      "1    103  Ferrari 458 Italia Convertible 2012  00002.jpg\n",
      "2    145                Jeep Patriot SUV 2012  00003.jpg\n",
      "3    187              Toyota Camry Sedan 2012  00004.jpg\n",
      "4    185             Tesla Model S Sedan 2012  00005.jpg\n",
      "0 195\n",
      "0 195\n",
      "0     13\n",
      "1      2\n",
      "2     90\n",
      "3    133\n",
      "4    105\n",
      "Name: class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# download the alternative excel file from kaggle\n",
    "excel_url = \"https://storage.googleapis.com/kaggle-forum-message-attachments/3037722/21362/stanford_cars_with_class_names.xlsx\"\n",
    "excel_data = pd.ExcelFile(excel_url)\n",
    "\n",
    "# read both excel sheets into df\n",
    "train_df = excel_data.parse(\"train\")\n",
    "test_df = excel_data.parse(\"test\")\n",
    "print(train_df.head())\n",
    "print(test_df.head())\n",
    "\n",
    "# drop unnecessary columns\n",
    "train_df = train_df.drop(columns=['Unnamed: 0', 'x1', 'y1', 'x2', 'y2'])\n",
    "test_df = test_df.drop(columns=['x1', 'y1', 'x2', 'y2'])\n",
    "test_df['image'] = test_df['image'].str.strip(\"'\")\n",
    "print(train_df.head())\n",
    "print(test_df.head())\n",
    "\n",
    "# labels start from 1, so subtract 1 to start from 0\n",
    "train_df['class'] = train_df['class'] - train_df['class'].min()\n",
    "test_df['class'] = test_df['class'] - test_df['class'].min()\n",
    "\n",
    "print(train_df['class'].min(), train_df['class'].max())\n",
    "print(test_df['class'].min(), test_df['class'].max())\n",
    "print(train_df['class'].head()) \n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-19T19:39:41.441349Z",
     "start_time": "2024-11-19T19:39:38.278538Z"
    }
   },
   "id": "c4f3aafc094b24a3",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Weh\\.cache\\kagglehub\\datasets\\jessicali9530\\stanford-cars-dataset\\versions\\2\n"
     ]
    }
   ],
   "source": [
    "path = dataset_download(\"jessicali9530/stanford-cars-dataset\")\n",
    "print(path)\n",
    "train_path = os.path.join(path, \"cars_train\", \"cars_train\")\n",
    "test_path = os.path.join(path, \"cars_test\", \"cars_test\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-19T19:39:45.246028Z",
     "start_time": "2024-11-19T19:39:44.446048Z"
    }
   },
   "id": "74833196e1c32b61",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class StanfordCarsDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_folder, image_size=(128, 128), transform=None):\n",
    "        self.image_folder = image_folder\n",
    "        self.image_size = image_size\n",
    "        self.transform = transform\n",
    "\n",
    "        # map image names to labels\n",
    "        self.mapping = {row['image']: row['class'] for _, row in dataframe.iterrows()}\n",
    "        self.image_paths = list(self.mapping.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_name = self.image_paths[idx]\n",
    "        label = self.mapping[image_name]\n",
    "        image_path = os.path.join(self.image_folder, image_name)\n",
    "\n",
    "        img = cv2.imread(image_path)\n",
    "        if img is None:\n",
    "            raise FileNotFoundError(f\"Error loading image: {image_path}\")\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, self.image_size)\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-19T19:39:46.707719Z",
     "start_time": "2024-11-19T19:39:46.700228Z"
    }
   },
   "id": "fa0c747ac94ac0f3",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# TODO: maybe augmentations on train data\n",
    "# transformations\n",
    "transform_rgb = transforms.Compose([\n",
    "    transforms.ToPILImage(),  # opencv to pil image\n",
    "    transforms.ToTensor(),    # convert image to tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # normalize for RGB (ResNet50)\n",
    "])\n",
    "\n",
    "transform_gray = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Grayscale(num_output_channels=1),  # grayscale\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])   # normalize for grayscale\n",
    "])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-19T19:39:51.003760Z",
     "start_time": "2024-11-19T19:39:50.995165Z"
    }
   },
   "id": "5ef331f7dbfa871b",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# datasets\n",
    "train_dataset = StanfordCarsDataset(train_df, train_path, image_size=(128, 128), transform=transform_rgb)\n",
    "test_dataset = StanfordCarsDataset(test_df, test_path, image_size=(128, 128), transform=transform_rgb)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-19T19:39:53.206749Z",
     "start_time": "2024-11-19T19:39:52.325560Z"
    }
   },
   "id": "ad9027a85791840a",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# train/val split: 80% train, 20% val\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset_split, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "batch_size = 32\n",
    "# dataloader\n",
    "train_loader = DataLoader(train_dataset_split, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-19T19:39:59.827248Z",
     "start_time": "2024-11-19T19:39:59.808203Z"
    }
   },
   "id": "ad0f073cb3c31bc3",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset:\n",
      "Train Dataset Index 0, Label: 13\n",
      "Train Dataset Index 1, Label: 2\n",
      "Train Dataset Index 2, Label: 90\n",
      "Train Dataset Index 3, Label: 133\n",
      "Train Dataset Index 4, Label: 105\n",
      "Train Dataset Index 5, Label: 122\n",
      "Train Dataset Index 6, Label: 88\n",
      "Train Dataset Index 7, Label: 95\n",
      "Train Dataset Index 8, Label: 166\n",
      "Train Dataset Index 9, Label: 57\n",
      "\n",
      "Test Dataset:\n",
      "Test Dataset Index 0, Label: 180\n",
      "Test Dataset Index 1, Label: 102\n",
      "Test Dataset Index 2, Label: 144\n",
      "Test Dataset Index 3, Label: 186\n",
      "Test Dataset Index 4, Label: 184\n",
      "Test Dataset Index 5, Label: 77\n",
      "Test Dataset Index 6, Label: 117\n",
      "Test Dataset Index 7, Label: 164\n",
      "Test Dataset Index 8, Label: 31\n",
      "Test Dataset Index 9, Label: 59\n",
      "Train Labels Range: 0 to 195\n",
      "Test Labels Range: 0 to 195\n",
      "\n",
      "Train DataLoader:\n",
      "Train Batch Labels: [13, 2, 90, 133, 105, 122, 88, 95, 166, 57]\n",
      "Shape of X [N, C, H, W]: torch.Size([32, 3, 128, 128])\n",
      "Shape of y: torch.Size([32]) torch.int64\n",
      "2 193\n",
      "\n",
      "Test DataLoader:\n",
      "Test Batch Labels: [180, 102, 144, 186, 184, 77, 117, 164, 31, 59]\n",
      "Shape of X [N, C, H, W]: torch.Size([32, 3, 128, 128])\n",
      "Shape of y: torch.Size([32]) torch.int64\n",
      "3 194\n",
      "Global Min Label: 0, Global Max Label: 195\n"
     ]
    }
   ],
   "source": [
    "# Training Dataset\n",
    "print(\"Train Dataset:\")\n",
    "for i in range(10):\n",
    "    _, label = train_dataset[i]\n",
    "    print(f\"Train Dataset Index {i}, Label: {label}\")\n",
    "\n",
    "# Test Dataset\n",
    "print(\"\\nTest Dataset:\")\n",
    "for i in range(10):\n",
    "    _, label = test_dataset[i]\n",
    "    print(f\"Test Dataset Index {i}, Label: {label}\")\n",
    "print(f\"Train Labels Range: {min(train_dataset.labels)} to {max(train_dataset.labels)}\")\n",
    "print(f\"Test Labels Range: {min(test_dataset.labels)} to {max(test_dataset.labels)}\")\n",
    "\n",
    "# Training DataLoader\n",
    "print(\"\\nTrain DataLoader:\")\n",
    "for X, y in train_loader:\n",
    "    print(f\"Train Batch Labels: {y[:10].tolist()}\")\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    print(y.min().item(), y.max().item())\n",
    "    break\n",
    "\n",
    "# Test DataLoader\n",
    "print(\"\\nTest DataLoader:\")\n",
    "for X, y in test_loader:\n",
    "    print(f\"Test Batch Labels: {y[:10].tolist()}\")\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    print(y.min().item(), y.max().item())\n",
    "    break\n",
    "\n",
    "batch_min = float('inf')\n",
    "batch_max = float('-inf')\n",
    "for _, labels in train_loader:\n",
    "    batch_min = min(batch_min, labels.min().item())\n",
    "    batch_max = max(batch_max, labels.max().item())\n",
    "\n",
    "print(f\"Global Min Label: {batch_min}, Global Max Label: {batch_max}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-18T17:23:59.501634Z",
     "start_time": "2024-11-18T17:22:29.256818Z"
    }
   },
   "id": "d4bbe409d262ab47",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to C:\\Users\\Weh/.cache\\torch\\hub\\checkpoints\\resnet50-11ad3fa6.pth\n",
      "100%|██████████| 97.8M/97.8M [00:03<00:00, 26.9MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet50TransferLearning(\n",
      "  (resnet): ResNet(\n",
      "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (4): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (5): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "      (1): LeakyReLU(negative_slope=0.01)\n",
      "      (2): Dropout(p=0.2, inplace=False)\n",
      "      (3): Linear(in_features=512, out_features=196, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# model: ResNet50, pretrained on ImageNet\n",
    "class ResNet50TransferLearning(nn.Module):\n",
    "    def __init__(self, num_classes=196, dropout_rate=0.2):\n",
    "        super(ResNet50TransferLearning, self).__init__()\n",
    "        self.resnet = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "\n",
    "        # adapt output layer to number of classes\n",
    "        num_ftrs = self.resnet.fc.in_features\n",
    "        self.resnet.fc = nn.Sequential(\n",
    "            nn.Linear(num_ftrs, 512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "model = ResNet50TransferLearning(num_classes=196, dropout_rate=0.2).to('cpu')\n",
    "print(model)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-19T19:41:55.489700Z",
     "start_time": "2024-11-19T19:41:50.330885Z"
    }
   },
   "id": "7bf9f2db731fdb70",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Loss and Optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, momentum=0.9, weight_decay=1e-4)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-19T19:41:58.889307Z",
     "start_time": "2024-11-19T19:41:58.883260Z"
    }
   },
   "id": "e4cb32a0e0472d00",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "'''\n",
    "# train loop\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to('cpu'), y.to('cpu')\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "# test loop\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to('cpu'), y.to('cpu')\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_loader, model, loss_fn, optimizer)\n",
    "    test(test_loader, model, loss_fn)\n",
    "print(\"Training complete!\")\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-18T17:24:04.148083Z",
     "start_time": "2024-11-18T17:24:04.139294Z"
    }
   },
   "id": "b21d761b6ec778a4",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# train and val loop (0.2 validation split)\n",
    "def train_and_validate(train_loader, val_loader, model, loss_fn, optimizer, epochs=10):\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        print(\"-------------------------------\")\n",
    "        \n",
    "        # train\n",
    "        model.train()\n",
    "        for batch, (X, y) in enumerate(train_loader):\n",
    "            X, y = X.to('cpu'), y.to('cpu')\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y)\n",
    "            \n",
    "            # backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch % 100 == 0:\n",
    "                loss, current = loss.item(), (batch + 1) * len(X)\n",
    "                print(f\"loss: {loss:>7f}  [{current:>5d}/{len(train_loader.dataset):>5d}]\")\n",
    "        \n",
    "        # validate\n",
    "        model.eval()\n",
    "        val_loss, correct = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for X, y in val_loader:\n",
    "                X, y = X.to('cpu'), y.to('cpu')\n",
    "                pred = model(X)\n",
    "                val_loss += loss_fn(pred, y).item()\n",
    "                correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        correct /= len(val_loader.dataset)\n",
    "        print(f\"Validation Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {val_loss:>8f} \\n\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-19T19:42:03.983052Z",
     "start_time": "2024-11-19T19:42:03.974118Z"
    }
   },
   "id": "7aa9db844f3b5b40",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# evaluate on test data\n",
    "def evaluate_on_test(test_loader, model, loss_fn):\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_loader:\n",
    "            X, y = X.to('cpu'), y.to('cpu')\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    \n",
    "    test_loss /= len(test_loader)\n",
    "    correct /= len(test_loader.dataset)\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-19T19:42:09.429121Z",
     "start_time": "2024-11-19T19:42:09.422078Z"
    }
   },
   "id": "3ba3784ca83ba88a",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "-------------------------------\n",
      "loss: 5.283438  [   32/ 6515]\n",
      "loss: 5.281435  [ 3232/ 6515]\n",
      "loss: 5.275332  [ 6432/ 6515]\n",
      "Validation Error: \n",
      " Accuracy: 0.7%, Avg loss: 5.274061 \n",
      "\n",
      "Epoch 2/10\n",
      "-------------------------------\n",
      "loss: 5.254241  [   32/ 6515]\n",
      "loss: 5.248442  [ 3232/ 6515]\n",
      "loss: 5.244256  [ 6432/ 6515]\n",
      "Validation Error: \n",
      " Accuracy: 1.0%, Avg loss: 5.259780 \n",
      "\n",
      "Epoch 3/10\n",
      "-------------------------------\n",
      "loss: 5.220606  [   32/ 6515]\n",
      "loss: 5.226896  [ 3232/ 6515]\n",
      "loss: 5.175627  [ 6432/ 6515]\n",
      "Validation Error: \n",
      " Accuracy: 2.0%, Avg loss: 5.237035 \n",
      "\n",
      "Epoch 4/10\n",
      "-------------------------------\n",
      "loss: 5.147103  [   32/ 6515]\n",
      "loss: 5.109557  [ 3232/ 6515]\n",
      "loss: 5.094997  [ 6432/ 6515]\n",
      "Validation Error: \n",
      " Accuracy: 2.8%, Avg loss: 5.182463 \n",
      "\n",
      "Epoch 5/10\n",
      "-------------------------------\n",
      "loss: 5.118543  [   32/ 6515]\n",
      "loss: 5.055025  [ 3232/ 6515]\n",
      "loss: 4.909935  [ 6432/ 6515]\n",
      "Validation Error: \n",
      " Accuracy: 3.5%, Avg loss: 5.047059 \n",
      "\n",
      "Epoch 6/10\n",
      "-------------------------------\n",
      "loss: 4.855048  [   32/ 6515]\n",
      "loss: 4.862733  [ 3232/ 6515]\n",
      "loss: 4.605947  [ 6432/ 6515]\n",
      "Validation Error: \n",
      " Accuracy: 6.4%, Avg loss: 4.779355 \n",
      "\n",
      "Epoch 7/10\n",
      "-------------------------------\n",
      "loss: 4.572895  [   32/ 6515]\n",
      "loss: 4.429509  [ 3232/ 6515]\n",
      "loss: 4.404064  [ 6432/ 6515]\n",
      "Validation Error: \n",
      " Accuracy: 9.9%, Avg loss: 4.456942 \n",
      "\n",
      "Epoch 8/10\n",
      "-------------------------------\n",
      "loss: 4.427655  [   32/ 6515]\n",
      "loss: 4.210156  [ 3232/ 6515]\n",
      "loss: 3.960068  [ 6432/ 6515]\n",
      "Validation Error: \n",
      " Accuracy: 12.5%, Avg loss: 4.110843 \n",
      "\n",
      "Epoch 9/10\n",
      "-------------------------------\n",
      "loss: 3.736271  [   32/ 6515]\n",
      "loss: 3.565827  [ 3232/ 6515]\n",
      "loss: 3.492398  [ 6432/ 6515]\n",
      "Validation Error: \n",
      " Accuracy: 16.6%, Avg loss: 3.727922 \n",
      "\n",
      "Epoch 10/10\n",
      "-------------------------------\n",
      "loss: 3.442417  [   32/ 6515]\n",
      "loss: 3.328346  [ 3232/ 6515]\n",
      "loss: 2.741424  [ 6432/ 6515]\n",
      "Validation Error: \n",
      " Accuracy: 20.9%, Avg loss: 3.409653 \n",
      "Test Error: \n",
      " Accuracy: 21.4%, Avg loss: 3.397093 \n"
     ]
    }
   ],
   "source": [
    "train_and_validate(train_loader, val_loader, model, loss_fn, optimizer, epochs=10)\n",
    "\n",
    "evaluate_on_test(test_loader, model, loss_fn)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-19T20:55:18.791960Z",
     "start_time": "2024-11-19T19:42:17.523041Z"
    }
   },
   "id": "383131f533047385",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model_val.pth\")\n",
    "#print(\"Saved PyTorch Model State to model.pth\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-19T20:55:28.001457Z",
     "start_time": "2024-11-19T20:55:27.740339Z"
    }
   },
   "id": "78d87a7f3d1f613a",
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Hyperparameter Tuning"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a0cad28e214ae510"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import random\n",
    "from itertools import product\n",
    "\n",
    "# parameter grid\n",
    "param_grid = {\n",
    "    \"lr\": [1e-4, 1e-3, 1e-2],\n",
    "    \"batch_size\": [16, 32, 64],\n",
    "    \"dropout\": [0.2, 0.3, 0.5],\n",
    "    \"optimizer\": [\"SGD\", \"Adam\", \"RMSprop\"],\n",
    "    \"weight_decay\": [1e-4, 1e-3]\n",
    "}\n",
    "\n",
    "# combinations\n",
    "all_combinations = list(product(*param_grid.values()))\n",
    "random_combinations = random.sample(all_combinations, 10)  # choose 10 random combinations\n",
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "for idx, (lr, batch_size, dropout, optimizer_name, weight_decay) in enumerate(random_combinations):\n",
    "    print(f\"Testing combination {idx + 1}: lr={lr}, batch_size={batch_size}, dropout={dropout}, optimizer={optimizer_name}, weight_decay={weight_decay}\")\n",
    "    \n",
    "    # dataloader with new batch size\n",
    "    train_loader = DataLoader(train_dataset_split, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "    model = ResNet50TransferLearning(num_classes=196, dropout_rate=dropout).to('cpu')\n",
    "\n",
    "    # optimizer\n",
    "    if optimizer_name == \"SGD\":\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
    "    elif optimizer_name == \"Adam\":\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    elif optimizer_name == \"RMSprop\":\n",
    "        optimizer = torch.optim.RMSprop(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    # train and validate\n",
    "    validation_accuracy = train_and_validate(train_loader, val_loader, model, loss_fn, optimizer, epochs=3)\n",
    "\n",
    "    # save results\n",
    "    results.append({\n",
    "        \"lr\": lr,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"dropout\": dropout,\n",
    "        \"optimizer\": optimizer_name,\n",
    "        \"weight_decay\": weight_decay,\n",
    "        \"validation_accuracy\": validation_accuracy\n",
    "    })\n",
    "\n",
    "# find best hyperparameter combination\n",
    "best_result = max(results, key=lambda x: x['validation_accuracy'])\n",
    "print(f\"Best Hyperparameter Combination: {best_result}\")\n",
    "\n",
    "# test best model on test data\n",
    "print(\"Evaluating the best model on test data...\")\n",
    "train_loader = DataLoader(train_dataset_split, batch_size=best_result['batch_size'], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=best_result['batch_size'], shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=best_result['batch_size'], shuffle=False)\n",
    "\n",
    "model = ResNet50TransferLearning(num_classes=196, dropout_rate=best_result['dropout']).to('cpu')\n",
    "if best_result['optimizer'] == \"SGD\":\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=best_result['lr'], momentum=0.9, weight_decay=best_result['weight_decay'])\n",
    "elif best_result['optimizer'] == \"Adam\":\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=best_result['lr'], weight_decay=best_result['weight_decay'])\n",
    "elif best_result['optimizer'] == \"RMSprop\":\n",
    "    optimizer = torch.optim.RMSprop(model.parameters(), lr=best_result['lr'], weight_decay=best_result['weight_decay'])\n",
    "\n",
    "# train and validate best model\n",
    "train_and_validate(train_loader, val_loader, model, loss_fn, optimizer, epochs=10)\n",
    "\n",
    "# evaluate on test data\n",
    "evaluate_on_test(test_loader, model, loss_fn)\n",
    "\n",
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"Saved PyTorch Model State to model.pth\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3986310cffea2d5b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
